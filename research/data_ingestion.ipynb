{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%pwd  # this tell us which path we are currently working , so based on the below output path we are working under the research file\n",
    "os.chdir(\"C:\\datascience End to End Projects\\End-to-End-Heart-Disease-Application-\")  #  but i would like to work with main ProjectML_with_MLFlow file , so for getting i step back in path inorder to enter the main project file i used this command os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is called the entity \n",
    "from dataclasses import dataclass # here i imported the dataclass from the dataclasses\n",
    "from pathlib import Path  # here i imported path from pathlib\n",
    "\n",
    "# here entity means DataIngestionConfig which it returns all the variables like root_dir,source_URL  and etc \n",
    "@dataclass(frozen=True) # here i declared the dataclass decorator\n",
    "class DataIngestionConfig:  # here i have created a class and named as DataIngestionConfig ,and it is not a python class because we need to declare the self to the variables if it is a python class, it is data class  and whenever i define the configuration fucntion , this class should my return function , the below are the varaible it do return \n",
    "    root_dir: Path    # these are variable which i have declared inside the class \n",
    "    source_URL: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path\n",
    "    train_csv: Path\n",
    "    test_csv: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"config/config.yaml\") # here iam returning config.yaml file and CONFIG_FILE_PATH is the varaible which stores the path\n",
    "PARAMS_FILE_PATH = Path(\"params.yaml\")  # here iam returning params.yaml file\n",
    "SCHEMA_FILE_PATH = Path(\"schema.yaml\")  # here iam returning the schema.yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PROJECTML.constants import * # here iam importing everthing which is present in the constants->__init__.py file into inside the data_ingestion.ipynb\n",
    "from PROJECTML.utils.common import read_yaml, create_directories # here iam importing the read_yaml, create_directories which are presenting inside the utils,common files into PROJECTML in which the file is data_ingestion.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:  # here iam creating class called ConfigurationManager\n",
    "    def __init__( # inisde this class iam reading all the yaml files which iam calling it from constants->__init__.py file and iam mentioning inside the class varaiable \n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath) # and here iam giving read_yaml path here and iam giving the path after that then it will return all the configuration in the variable\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "# now i will create artifacts root in the side of the vscode project one of the path and the below i will define the data ingestion cofiguration function\n",
    "    # the above one  entity which inside 4 variables needs to return by this below fucntion\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir]) # here iam creating the root directory, and iam reading the config from the configurationManager class and iam going to access all the data ingestion from the config.yaml file \n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,  # that how iam accessing all the things like root_dir,source_url and etc from config.yaml file and finally this fucntion do return all this variables \n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir,\n",
    "            train_csv=config.train_csv,\n",
    "            test_csv=config.test_csv\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are libraries i need for to uodate the components \n",
    "import os\n",
    "import urllib.request as request # so i use the request to download the data from the URL\n",
    "import zipfile # here iam using the Zipfile to transform the data \n",
    "from PROJECTML import logger # here logger is used to logger the data \n",
    "from PROJECTML.utils.common import get_size # here i used the getsize is used to get to know the file size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# iam going to define one class which is DataIngestion from that class which it will take the DataIngestionConfig because from this dataingestionConfig only it will get to know the path \n",
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "# now i will define one method which it is responsible for dowmloading the data \n",
    "    def download_file(self):\n",
    "        if not os.path.exists(self.config.local_data_file):\n",
    "            filename, headers = request.urlretrieve(\n",
    "                url = self.config.source_URL, # it will download the dta from this URL\n",
    "                filename = self.config.local_data_file\n",
    "            )\n",
    "            logger.info(f\"{filename} download! with following info: \\n{headers}\")\n",
    "        else:\n",
    "            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\") # if the data file is already exist it will print the message like that data file is already exit\n",
    "\n",
    "    \n",
    "\n",
    "# now iam going to another method called ExtractZipfile\n",
    "    def extract_zip_file(self):\n",
    "        \"\"\"\n",
    "        zip_file_path: str\n",
    "        Extracts the zip file into the data directory\n",
    "        Function returns None\n",
    "        \"\"\"\n",
    "        unzip_path = self.config.unzip_dir\n",
    "        os.makedirs(unzip_path, exist_ok=True)\n",
    "        with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref: # here it will take the local_data_file path which is present in the config.yaml  local_data_file: artifacts/data_ingestion/data.zip and it will unzip the folder to this data_ingestion \n",
    "            zip_ref.extractall(unzip_path)\n",
    "\n",
    "\n",
    "    def train_test_spliting(self):\n",
    "\n",
    "        data=pd.read_csv(\"artifacts\\data_ingestion\\Heart_csv\\heart.csv\")\n",
    "        # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "        train, test = train_test_split(data, test_size=0.20,random_state=2)\n",
    "\n",
    "        train.to_csv(os.path.join(self.config.root_dir, \"train.csv\"), index=False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir, \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-26 18:45:34,912: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-02-26 18:45:34,917: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-02-26 18:45:34,922: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-02-26 18:45:34,924: INFO: common: created directory at: artifacts]\n",
      "[2024-02-26 18:45:34,926: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "[2024-02-26 18:45:34,927: INFO: 3653406326: File already exists of size: ~ 9 KB]\n"
     ]
    }
   ],
   "source": [
    "# Now iam going to Update my pipeline First iam Initilizing my ConfigirationManager and from this COnfigrationManager iam calling my DataIngestionConfig and this thing iam returning it my dataingestionclass because it will take the data ingestion config  and this thing iam returning it my dataingestionclass because it will take the data ingestion config \n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config) # here iam passing my dataingestionconfig\n",
    "    data_ingestion.download_file() # here iam downloading the file\n",
    "    data_ingestion.extract_zip_file() # here iam extracting the zip file , and here we following it by step by step thats why we call it as pipeline \n",
    "    data_ingestion.train_test_spliting()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
